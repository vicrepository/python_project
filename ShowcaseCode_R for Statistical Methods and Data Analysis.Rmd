--- 
title: "Coursework for Statistical Methods and Data Analysis"
Author: F121092
output:
  html_document:
    df_print: paged
---

# R-notebook settings

```{r}
knitr::opts_chunk$set(
  comment = "",
  results = "hold",
  echo = TRUE,
  message = FALSE,
  warning = FALSE,
  fig.width = 4,
  fig.height = 2.6,
  fig.align = "center"
)
```

# Packages 

```{r, message = FALSE, warning = FALSE}

library("car")
library("tidyverse")
library("magrittr")
library("here")
library("janitor")
library("lubridate")
library("gridExtra")
library("readxl")
library("glmnet")
library("Lahman")
library("viridis")
library("lindia")
library("lme4")
library("caret")
library("pROC")
```

# 1. Datesets

## a. Create a new dataset called 'Peopledata' that contains all of the variables in the 'People' dataset by
### i. removing all birth information except birthYear and birthCountry and all death information, along with the variable finalGame;
### ii. replacing birthCountry is by bornUSA, a logical variable indicating if the player was born in the USA.
```{r}

Peopledata <- People %>%
  select("playerID", "birthYear", "nameFirst", "nameLast", "weight", "height", "bats", "throws", "debut", "birthCountry") %>%
  mutate(bornUSA = birthCountry %in% "USA") %>%
  select(!birthCountry)

Peopledata  <- as.tibble(Peopledata)

Peopledata  #Check table

``````

## b. Create new datasets called Battingdata and Fieldingdata by 
### i. choosing data from the years 1985 and 2015,
### ii. selecting only those variables that for those years have fewer than 25 missing cases, 
### iii. removing the variable 'G' from the batting dataset and removing the variables "teamID" and "lgID" from both datasets, 
### iv. creating a variable in 'Battingdata' called batav which is equal to the number of hits (H) over the number of at bats (AB) if the number of hits >0, and =0 if H=0.
```{r}

#Creation of Batting data
Battingdata <- Batting %>%
  filter(yearID == 1985 | yearID == 2015) %>%
  select(!c(G,teamID, lgID)) %>%
  mutate(batav = case_when(H == 0 ~ 0, H > 0 ~  H / AB))

Battingdata %>% sapply(function(x) sum(is.na(x)) ) #No missing values in Battingdata, code used after filter() function
Battingdata <- as.tibble(Battingdata)

Battingdata

```
```{r}

#Creation of Fielding data
Fieldingdata <- Fielding %>%
  filter(yearID == 1985 | yearID == 2015) %>%
  select(!c(PB, WP, SB, CS, ZR)) %>%
  select(!c( teamID, lgID))

Fieldingdata  %>% sapply(function(x) sum(is.na(x)) ) #Found columns (PB, WP, SB, CS, ZR) >25 missing cases, code used after filter() function  

Fieldingdata<- as.tibble(Fieldingdata)

Fieldingdata
```

## c. Create a dataset 'Playerdata' from the dataset 'Salaries' by
### i. selecting data from the years 1985 and 2015,
### ii. adding all distinct variables from the Fieldingdata, Battingdata and Peopledata datasets,
### iii. creating a new variable 'allstar' indicating if the player appears anywhere in the AllstarFull dataset,
### iv. creating a new variable 'age' equal to each player's age in the relevant year,
### v. dropping unused levels of any categorical variable.

```{r}

Playerdata <- Salaries %>%
  filter(yearID == 1985 | yearID == 2015) %>%
  inner_join(Fieldingdata, 
            by = c('yearID', 'playerID'), 
            keep = FALSE) %>%
  
  mutate(allstar = playerID %in% AllstarFull$playerID) %>%
  inner_join(Battingdata, 
            by = c('yearID', 'playerID', 'stint'), 
            keep = FALSE) %>%
  inner_join(Peopledata, 
            by = c("playerID" = "playerID"), 
            keep = FALSE) %>%
  mutate(age = yearID - birthYear) %>%
  drop_na %>%
  droplevels() 

levels(Playerdata$bats) #Checked there were 3, maintained after dropping
levels(Playerdata$throws) #Checked there were 3 levels and now 2 

Playerdata <- as.tibble(Playerdata)

Playerdata

```

## d. Create a dataset called 'TeamSalaries' in which there is a row for each team and each year and the variables are:
### i. 'Rostercost' = the sum of all player salaries for the given team in the given year
### ii. 'meansalary' = the mean salary for that team that year
### iii. 'rostersize' = the number of players listed that year for that team.

```{r}
#i
 TeamSalaries <- Salaries %>%
  group_by(teamID, yearID) %>%
  summarise(Rostercost = sum(salary), meansalary = mean(salary), rostersize = n_distinct(playerID)) 

TeamSalaries

```

## e. Create a dataset 'Teamdata' by taking the data from the Teams dataset for the years 1984 to 2016, inclusive and adding to that data the variables in TeamSalaries. Drop any incomplete cases from the dataset.
```{r}

Teamsdata <- Teams %>%
  filter(yearID >= 1984, yearID <= 2016) %>%
  inner_join(TeamSalaries, by = c('yearID', 'teamID'), 
            keep = FALSE) %>%
  drop_na()

Teamsdata

```
# 2. Simple Linear Regression

## a.  [2 + 2 points] Create one plot of mean team salaries over time from 1984 to 2016, and another of the log base 10 of team mean salaries over time from 1984 to 2016.  Give two reasons why a linear model is more appropriate for log base 10 mean salaries than for raw mean salaries.

```{r}

Teams_wrong <- TeamSalaries %>%
  inner_join(Teams, by = c('yearID', 'teamID')) %>%
  mutate(log10_meansalary = log10(meansalary)) %>%
  filter(yearID >= 1984, yearID <= 2016) 



Teamsdata %>%
  ggplot(mapping = aes(x = yearID, y = meansalary)) +
  geom_point()+
  labs(x = "Year", y = "Meansalary") +
  ggtitle("MeanSalary VS Year") +
  geom_smooth(method = "lm", se = FALSE,colour="red") +
  theme_classic()
  
```

```{r}

Team_new <- Teamsdata %>%
  mutate(log10_meansalary = log10(meansalary))



Team_new %>%
  mutate(log10_meansalary = log10(meansalary)) %>%
  ggplot(mapping = aes(x = yearID, y = log10_meansalary)) +
  geom_point()+
  labs(x = "Year", y = "Meansalary") +
  ggtitle("MeanSalary VS Year") +
  geom_smooth(method = "lm", se = FALSE,colour="red") +
  theme_classic()
  
```
Reasons why second graph is better:

1. After comparing the 2 graphs (MeanSalary VS Year & LogMeanSalary VS Year), we can see that the spread above and below the regression line on the LogMeanSalary graph indicating a more homoscedascedity graph.

2. there is also less dispersion display when compared. Noted that we can see increase in dispersion on the MeanSalary graph when Year increases.

## b. [1 + 3 points] Fit a model of $log_{10}$(meansalary) as a function of yearID.  Write the form of the model and explain what the Multiple R-Squared tells us.

```{r}

lm_teamsal <- lm(log10_meansalary  ~ yearID ,data = Team_new )
summary(lm_teamsal)

```
$$
\begin{align}
\mbox{log10(MeanSalary)} =  -61.6 + 0.0387 \times {\rm Year} 
\end{align}
$$
Multiple R-squared = 0.4878
This represents that estimate of 49% of the variances of log MeanSalary are explained by the yearID.


## c.  [1 + 8 points] State and evaluate the four assumptions of linear models for this data.

```{r}

lm_teamsal %>%
 gg_diagnose(max.per.page = 1)

```
Four assumptions for linear regression model:
-Normality 
Lets first consider the histogram and QQ plot for the residuals. We can see the histogram is displaying a bell-shape where there are more counts at the center and less counts on both sides evenly, it looks roughly Gaussian distributed. QQ plot also reinforces this finding, the scatter values lie along the line of theoretical distribution which again, supports the normality assumption where the data is normally distributed.

-Linearity
The scatter plot for log10(MeanSalary) versus year was roughly linear. The residuals versus fitted value and residuals versus year graphs also showed a linear pattern where the points lay around the 0.0 residuals value.

-Homoscedascitiy 
The scatter of residuals versus year and residuals versus fitted, are roughly (although there could be a slight trend where the spread increase from left to right) the same width across the plot and which doesn’t show any indication of trend.

-Independence
Points does appears to show some indications of under autocorreclation when we look at the graphs of residuals versus year. This means that standard errors in the  output we cannot replied on but further tests such as Durbin-Wastson test and Breusch-Godfrey test can be used to confirm this.


## d.  [3 + 1 points] Plot confidence and prediction bands for this model.  Colour the points according to who won the World Series each year.  Comment on what you find.

```{r}
#Confidence and prediction band

predsal <- predict(lm_teamsal, interval="prediction") %>%
  as.data.frame()
myTeams_new <- cbind(Team_new, predsal)


ggplot(myTeams_new ,aes(yearID,log10_meansalary, colour = WSWin)) +
  geom_point(size=2)+
  geom_smooth(method=lm, color='#2C3E50')+
  geom_line(aes(y=lwr), color=2,lty=2) +
  geom_line(aes(y=upr), color=2,lty=2) +
  labs( 
     title = "Log(MeanSalary) VS Year", 
    x = "Year", 
    y = "Log10 (MeanSalary)" 
  )


```
## e. [1 + 1 points] Investigate the points that appear above the top prediction band.  What team or teams do they relate to?

```{r}

myTeams_new %>%
  filter(log10_meansalary > upr) %>%
  count(teamID)

```
Team NYA (9 counts), appears to have a log10(meansalary) higher than the prediction band in the year between 1984 to 2016.


# 3. Multiple regression for Count Data

## a. [2 + 2 points] Create a histogram of the number of runs scored for players in the Playerdata dataset so each bar is a single value (0,1,2 runs, etc).  Next create a histogram of the number of runs for all players who have had a hit. Give a domain-based and a data-based reason why it is more reasonable to create a Poisson data for the second set than the first.  

```{r}

#Histogram of the number of runs scored for players

Playerdata %>%
  select(yearID, playerID, R) %>%  
  distinct()%>% 
   ggplot(aes(R)) +
  geom_histogram(binwidth = 1) +
  labs( 
     title = "Hisogram for number of runs scored for players", 
    x = "Number of Runs scored for players", 
    y = "Count" 
  )

#Noted that players will be playing different roles in the a game but end score of a match or matches will be the same. This creates extra counts and hence is worth removing the duplication as to answer the question 3a

```

```{r}

#Histogram of the number of runs for all players who have had a hit

Playerdata %>%
  select(yearID, playerID, R, H) %>%  
  distinct()%>%
  filter(H >= 1)%>%
   ggplot(aes(R)) +
  geom_histogram(binwidth = 1)  +
  labs( 
     title = "Histogram of the number of runs scored for all players who have had a hit", 
    x = "Number of runs for all players who have had a hit", 
    y = "Count" 
  )

#Noted that players will be playing different roles in the a game but end score of a match or matches will be the same. This creates extra counts and hence is worth removing the duplication as to answer the question 3a

```
Players will only run after they had hit a ball. Therefore, there is no point including players who do not hit the ball (i.e. H = 0) in a histogram where we are count number of run scored of the players when some of them will never score. Including players with zero hit could result in is zero inflation in the data, hence using poisson model for the first histogram will not be appropriate.

## b.  [3 + 0 points] Create a new dataset, OnBase of all players who have had at least one hit.  Transform yearID to a factor.  Construct a Poisson model, glm1, of the number of runs as a function of the number of hits, the year as a factor, position played and player height and age.

```{r}

OnBase <- Playerdata %>%
  filter(H >= 1) %>%
  mutate(yearID = as.factor(yearID))

#As we are trying to produce a model seeing if position could acts as a predictor for run score, there is no need the distinct process as mentioned in 3a.

glm1_withP <- glm(R ~ H + yearID + POS + height + age ,data = OnBase,family="poisson") #If a POS (only categorical variable in here) is accumulating in zero, it would badly impact model

glm1_withP %>%
ggplot(aes(POS, R))+
  geom_boxplot()+
  geom_hline(yintercept=2,col="red")


```
Seems like majority of POS = "P" do not really has any run score.


```{r}
#Investigating the Run score and Hit relationship
glm1_withP %>% ggplot(aes(x=H,y=R,colour = factor(POS)))+
  geom_point()
```
We can see position P clusters at zero hit and zero run.

Lots of P are accumulating in H = 0, R = 0

```{r}

plot(glm1_withP,which=3)
abline(h=0.8,col=3)

```
seems we may be running into trouble due to the POS = P, clustering separately to the other group in the left hand side. They are really not relevant to our Running score activities. 

Lets create a Poisson model without P:
```{r}

glm1 <- glm(R ~ H + yearID + POS + height + age ,data = OnBase[!OnBase$POS == "P",],family="poisson") #Position "P" is removed due to the role do not require them them to run and score
summary(glm1)
```
## c.  [2 + 4 points] Find the p-value for each of the predictor variables in this model using a Likelihood Ratio Test.  What hypothesis does each p-value test, and what mathematically does a p-value tell you about a variable?  Use this definition to say what is meant by the p-value associated to POS and to the p-value associated to height.

```{r}
Anova(glm1)
```
P values for:
Number of hit scored in game (H) = 2.2e-16
year (yearID) = 0.35656 
Position (POS) = 5.75e-13
Height (height) = 0.74369
Age (age) = 0.05939


In statistic, P values is used in null hypothesis significance testing. P-value below 0.05 indicates the parameter is statistically significant and in here, will act as an important predictor of the response variable.

From the above Deviance Table from glm1, we can notice that 2 variables, H & POS(i.e Number of hit in game and position of player) are below 0.05 or in fact, are all below 0.001. This indicates the results were highly significant and are important predictors.

P-value of POS suggests that it is an important predictor when comparing the model with variables POS + height + age + yearID to another model with variables height + age + yearID, . Similar comment can be made for p-value associated to height but in opposite. un-statistically significant p-value of height suggests that it is not an important predictor when comparing the model with variables POS + height + age + yearID to another model with variables POS + age + yearID. 

## d. [1 + 8 points] State the assumptions of Poisson models and check these where possible.

-The response variable has to be a count. 
The response variable of number of runs for player who have had a hit is a count where only whole number is counted in (i.e. 1,2,3 etc) and not a fraction. We used "Histogram of the number of runs for all players who have had a hit" to display that each column represent a whole number.

```{r}
class(OnBase$H) #Check if it is integer
```

-The expected value of response variable x has to be greater or equal to zero, so is E(x) of course.
Number of runs for player who have had a hit will not have be negative, as showed in the "Histogram of the number of runs for all players who have had a hit" shows that no column is counting negative number of 1 of below zero.

```{r}
min(OnBase$H) #Check if greater than 0, i.e. positive number
```

-Assumption of variance = mean is reasonable for this dataset. We can use the plot of the absolute value of residuals versus predicted means, which should look flat, and hover around 0.8(green line).

```{r}
plot(glm1,which=3) +
abline(h=0.8,col=3)
```
The red line is not flat, and it is above the 0.8. This suggests that a slight overdispersion in the data that increases linearly as the prediction increases. This situation is very common when we have not accounted for all of the important predictors in the model and in this case, it is not so great.

-Linearity: 
Evaluate using (deviance) residuals vs fitted and see if its fairly flat. 
```{r}
plot(glm1,which=1) 
```
when comparing the red line to the black line, We can see the data points are not flat in the graph and showing a inverted parabola shape. We have compromised linearity in here. 

-Distribution: 
For deviance residuals, we again investigate the qqplot. 
```{r}
plot(glm1,which=2) 
```
This is showing a very nice straight line. Hence, we don't need to use robust confidence intervals later.

-Independence:
We need to investigate (deviance) residuals as a function of order of datapoints and look for evidence of “snaking”. We don’t have a natural order in this dataset, so we can’t investigate in here.

## e. [2 + 4 points] Now create a new model that includes teamID as a random effect.  Ensure there are no fit warnings.  What does the result tell us about the importance of team on number of runs that players score?  Is this a relatively large or small effect?  How could we check the statistical significance of this effect in R?

```{r}
glmer1 <-glmer(R ~ H + yearID + POS + height + age + (1|teamID),data = OnBase[!OnBase$POS == "P",],family="poisson", nAGQ=0)


summary(glmer1)

```
```{r}



```


As we are including teamID as a random effect in our model, there is no coefficient of teamID anymore. Instead, we have Tau τ.
Tau τ, standard deviation of random effect, i.e. teamID, is 0.09493.
This tells us that:

$$
\begin{align}

  \mbox{Number of runs that players score, R} \sim {\rm pois}(2.517415 + 0.012758 \times H + 0.018005 \times yearID2015 \\
  -0.008009 \times POS2B + 0.008517 \times POS3B -0.067949 \times POSC \\-0.011937 \times POSSS  -0.002742 \times height + 0.002092 \times age + {\rm u})
  \\ {\rm u} \sim N(0,0.09493)
  
\end{align}
$$


Lets undo the log link function to look at how many more times number of runs that players score for POS = 1B, rather than how many numbers of runs that players score all together you would expect. Noted that POS = 1B is the base level.

calculation 95% confident intervals from this SD:
±1.96*sd is the 95% confident intervals = ±1.204498

```{r}
exp(1.96*0.09493)

#So just being in the top team in opposed to the average team for POS = 1B, means you will score +1.204498 times the mean Number of runs that players score, i.e around 20% more scores

```

```{r}
1 - (1 / exp(1.96*0.09493))

#And being in the worst team for POS = 1B, you will score 1.204498 times less the mean Number of runs that players score, i.e. around 17% less scores.
```
And when we check the dispersion of the model:

```{r}

overdisp_fun <- function(model) {
rdf <- df.residual(model)
rp <- residuals(model,type="pearson")
Pearson.chisq <- sum(rp^2)
prat <- Pearson.chisq/rdf
pval <- pchisq(Pearson.chisq, df=rdf, lower.tail=FALSE)
c(chisq=Pearson.chisq,ratio=prat,rdf=rdf,p=pval)
}

overdisp_fun(glmer1)


```
we can see that p is <0.05, there is evidence of overdispersion 


## f. [2 + 0 points] What is the mean number of runs could you expect 30-year old, 72 inch tall outfielders playing for the Baltimore Orioles in 2015 with 20 hits to have scored? 

```{r}

predict(glmer1,newdata=data.frame(age = 30, height = 72, POS = "OF", teamID = "BAL", yearID = "2015", H = 20,type = "response"))

```
So we would predict this player will have 2.87.


# 4.  Lasso Regression for Logistic Regression

## a. [4 + 0 points] Create a new dataset DivWinners by removing all of the variables that are team or park identifiers in the dataset, as well as 'lgID', 'Rank','franchID','divID', 'WCWin','LgWin', and 'WSwin'. Split the resulting into a training and a testing set so that the variable 'DivWin' is balanced between the two datasets.  Use the seed 123.

```{r}
#Creation of Divwinners
Divwinners <- Teamsdata  %>%
  select( !starts_with('team')) %>%
  select(!starts_with('park')) %>%
  select(!c('lgID', 'Rank','franchID','divID', 'WCWin','LgWin','WSWin', 'name')) %>%
  drop_na() 
  
Divwinners

```


Set.seed(123) & creating training and testing data in a 80:20 split.

```{r}

set.seed(123)

training.samples <- Divwinners$DivWin %>%
  createDataPartition(p = 0.8, list = FALSE)
train.data <- Divwinners[training.samples, ]
test.data <- Divwinners[-training.samples, ]

```

## b.  [4 + 0 points] Use the training data to fit a logistic regression model using the 'glmnet' command.  Plot residual deviance against number of predictors.  

```{r}

DivWintrain <- as.vector(train.data$DivWin)

Divwinnerspredict <-model.matrix(~.-1,train.data[,c(-6)]) #Removing response variable column "DivWin" 

Divwinnersfit <- glmnet(Divwinnerspredict, DivWintrain, family = "binomial")

plot(Divwinnersfit,xvar="dev")  #Plot residual deviance against number of predictors

```

## c.  [2 + 2 points] How many nonzero model coefficients are needed to explain 50% of the deviance? 60%?  Which coefficients are these in each case?  

```{r}
#To figure out which coefficients are involved, print out the values of lambda compared to the % Deviance and Df 
Divwinnersfit
```

To explain 50% of variances:
In row number 21,df= 2, %Dev = 50.02 and lambda = 0.038030

To explain 60% of variances:
In row number 53,df= 26, %Dev = 60.13 and lambda = 0.001937

```{r}

#To explain 60% of deviance in the result we select lambda = 0.001937

Divwinnerscoef <- coef(Divwinnersfit, s=0.001937)
Divwinnerscoef@Dimnames[[1]][1+Divwinnerscoef@i]
```
After checking with the data set Divwinners, these are all quantitative variables.

```{r}
n_distinct(Divwinnerscoef@Dimnames[[1]][1+Divwinnerscoef@i])
```
Total of 27 non-zero coefficient.

## d.  [2 + 1 points] Now use cross-validation to choose a moderately conservative model.  State the variables you will include.

```{r}

set.seed(321)

cv <- cv.glmnet(Divwinnerspredict,DivWintrain, family = "binomial")
plot(cv)

```
We should include 4 to 6 variables as the range of Binomial Deviance is smallest in this range.
```{r}
Divwinners1se<-coef(Divwinnersfit,s=cv$lambda.1se)
setdiff(Divwinners1se@Dimnames[[1]][1+Divwinners1se@i],Divwinnerscoef@Dimnames[[1]][1+Divwinnerscoef@
i])

```
There is no set diff between using the s = 0.008584 which explain 60% variances in the data then using lambda = 1se

```{r}

Divwinnersmax<-coef(Divwinnersfit,s=cv$lambda.min)
Divwinnersmax@Dimnames[[1]][1+Divwinnersmax@i]

```
Now we have: 3 quantitative variable:"W" , "L" , "attendance" and their total levels is only 3.

I am selecting all 3 of them as their binomial deviance is small when we looked at the glmnet output as well as the cross-validation graph.
 
 
 

## e.  [4 + 2 points] Fit the model on the training data, then predict on the testing data.  Plot comparative ROC curves and summarise your findings.

```{r}
set.seed(123)
train.model <- glm(as.factor(DivWin) ~ W + L + attendance, data = train.data, family = "binomial")

predtrain<-predict(train.model,type="response")
predtest<-predict(train.model,newdata=test.data,type="response")

# Plotting the ROC curve

roctrain<-roc(response=train.data$DivWin,predictor=predtrain,plot=TRUE,main="ROC Curve for prediction of DivWinnder",auc=TRUE)
roc(response=test.data$DivWin,predictor=predtest,plot=TRUE,auc=TRUE,add=TRUE,col=2) 
legend(0,0.4,legend=c("training","testing"),fill=1:2)

```
These two curves are very similar in shape. This is a good sign that the model is not overfitted to the training data.

## f.  [4 + 2 points] Find Youden's index for the training data and calculate confusion matrices at this cutoff for both training and testing data.  Comment on the quality of the model for prediction in terms of false negative and false positive rates for the testing data.

```{r}
# Youden's Index for the training data
youdenN<-coords(roctrain,"b",best.method="youden",transpose=TRUE)
youdenN
youdenN[2]+youdenN[3] #Sum of sensitivity + specificity for Youden's Index
```
Youden's Index is 0.1836071 for the training data.

```{r}
# Calculate Confusion matrix for training data
train.data$predwins <- ifelse(predict(train.model,newdata=train.data, type="response")>= 0.1836071,"Y","N")
T1 <- table(train.data$predwins,as.factor(train.data$DivWin))
names(dimnames(T1))<- list("Predicted", "Actual")
T1
```
```{r}
sensitivity(T1)
specificity(T1) 
```
Note that this model is testing for "N".
Sensitivity for training data = 354/(354+64) = 0.84689
Specificity for training data = 100/(100+6) = 0.9433962

False positive rate for training data = 1-0.9433962 = 0.15311
False Negative rate for training data = 1-0.84689 = 0.0566038

```{r}
# Calculate Confusion matrix for testing data
test.data$predwins <- ifelse(predict(train.model,newdata=test.data, type="response")>= 0.1836071,"Y","N")
T2 <- table(test.data$predwins,as.factor(test.data$DivWin))
names(dimnames(T2))<- list("Predicted", "Actual")
T2
```
```{r}
sensitivity(T2) 
specificity(T2) 
```
Sensitivity for testing data = 87/(87+17) = 0.8365385
Specificity for testing data = 25/(25+1) = 0.9615385

False positive rate for testing data = 1-0.8365385 = 0.1634615
False Negative rate for testing data = 1-0.9615385 = 0.0384615

From the findings above, we can see the sensitivity and specificity are both very similar as well as high on the 2 data sets.
This implies that the false positive and false negative rate are low in the data sets using the model we built from training data.

## g.  [5 + 1 points] Calculate the sensitivity+specificity on the testing data as a function of divID and plot as a barchart.  Is the prediction equally good for all divisions?  

```{r}
train.model <- glm(as.factor(DivWin) ~ W + L + attendance, data = train.data, family = "binomial")

#Merging data with Teamsdata to regain divID column into the test.data

divID <- merge(x = test.data, y = Teamsdata[ , c("yearID","W","L","R","H","divID")], by = c("yearID","W","L","R","H"), all.x=TRUE) 

#There are 3 divID: "E" "W" "C"

#Begin with creating the sensitivity+specificity value for div_E
divID_E <- divID %>%
  filter(divID == "E")

Table_E <- table(divID_E$predwins,as.factor(divID_E$DivWin))
names(dimnames(Table_E))<- list("Predicted", "Actual")
Table_E #noted that confusion matrix is not necessary to display in here but included for better visualisation.

E_SS <- sensitivity(Table_E) + specificity(Table_E)
E_SS

```
```{r}

#Creating the sensitivity+specificity value for div_W
divID_W <- divID %>%
  filter(divID == "W")

Table_W <- table(divID_W$predwins,as.factor(divID_W $DivWin))
names(dimnames(Table_W))<- list("Predicted", "Actual")
Table_W

W_SS <- sensitivity(Table_W) + specificity(Table_W)
W_SS 

```

```{r}

#Creating the sensitivity+specificity value for div_C
divID_C <- divID %>%
  filter(divID == "C")

Table_C <- table(divID_C$predwins,as.factor(divID_C$DivWin))
names(dimnames(Table_C))<- list("Predicted", "Actual")
Table_C

C_SS <- sensitivity(Table_C) + specificity(Table_C)
C_SS
```


```{r}
#Creating a dataframe for the sensitivity+specificity with individual divID labelled

SnS <- c(E_SS, W_SS, C_SS)
DivID <- c("Div_E", "Div_W", "Div_C")

Sum_SnS <- data.frame(DivID, SnS)

ggplot() +
geom_col(mapping = aes(x = DivID,y = SnS))+
scale_y_continuous() +
  labs( 
  x = "DivID", 
  y = "Sensitivity + Specificity",  
  title = "Sensitivity + Specificity for individual DivID in testing data")

```
Although we can see that Div_W has the highest sensitivity+specificity value, followed by Div_C and Div E, they are actually very similar when we observe them from the bar chart. The difference between each DivID is really minimal. 

We can conclude that the sensitivity + specificity are fairly similar when we are applying the model into individual divID which means the quality of the model is equally good for each divID. 

